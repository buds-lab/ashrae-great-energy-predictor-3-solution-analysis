{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on https://www.kaggle.com/isaienkov/keras-nn-with-embeddings-for-cat-features-1-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this is a keras tensorflow so no need to change /.keras/keras.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./sample_submission.csv\n",
      "./submission_withoutleak001.csv.gz\n",
      "./submission_nn007lofo.csv.gz\n",
      "./generate_datasets.ipynb\n",
      "./rows_to_drop.csv\n",
      "./building_all_meters.feather\n",
      "./train_with_cnt_per_tmp_meter.feather\n",
      "./test_simple_cleanup.feather\n",
      "./site4.csv\n",
      "./winning sub.ipynb\n",
      "./level2--ensembling_model.ipynb\n",
      "./untitled.txt\n",
      "./submission_multimeter003.csv.gz\n",
      "./tests_building.ipynb\n",
      "./level1--submission_multimeter003.ipynb\n",
      "./site1.pkl\n",
      "./level1--submission_whatsyourcv3_0052_trncl.ipynb\n",
      "./weather_train.csv\n",
      "./generate_leak_data.ipynb\n",
      "./leak012345_001.feather\n",
      "./site15_leakage.csv\n",
      "./train.csv\n",
      "./submission_multimeter004_nobuild.csv.gz\n",
      "./test.csv\n",
      "./weather_test.csv\n",
      "./submission_ucf_replaced.csv\n",
      "./level1--submission_nn007lofo--CNN.ipynb\n",
      "./level1--submission_nn001-DenseNN.ipynb\n",
      "./submission_whatsyourcv3_0052_trncl.csv.gz\n",
      "./building_metadata.csv\n",
      "./asu_2016-2018.csv.zip\n",
      "./train_cleanup_001.feather\n",
      "./train_simple_cleanup.feather\n",
      "./level1--submission_multimeter004_nobuild.ipynb\n",
      "./level1--submission_withoutleak001.ipynb\n",
      "./.ipynb_checkpoints/level1--submission_nn007lofo--CNN-checkpoint.ipynb\n",
      "./.ipynb_checkpoints/generate_datasets-checkpoint.ipynb\n",
      "./.ipynb_checkpoints/level1--submission_nn001-DenseNN-checkpoint.ipynb\n",
      "./.ipynb_checkpoints/level1--submission_whatsyourcv3_0052_trncl-checkpoint.ipynb\n",
      "./.ipynb_checkpoints/level1--submission_multimeter003-checkpoint.ipynb\n",
      "./.ipynb_checkpoints/generate_leak_data-checkpoint.ipynb\n",
      "./.ipynb_checkpoints/tests_building-checkpoint.ipynb\n",
      "./.ipynb_checkpoints/level1--submission_withoutleak001-checkpoint.ipynb\n",
      "./.ipynb_checkpoints/level2--ensembling_model-checkpoint.ipynb\n",
      "./.ipynb_checkpoints/level1--submission_multimeter004_nobuild-checkpoint.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate, BatchNormalization, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.losses import mean_squared_error as mse_loss\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_imputation(df, column_name):\n",
    "    imputation = df.groupby(['timestamp'])[column_name].mean()\n",
    "    \n",
    "    df.loc[df[column_name].isnull(), column_name] = df[df[column_name].isnull()][[column_name]].apply(lambda x: imputation[df['timestamp'][x.index]].values)\n",
    "    del imputation\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "building_df = pd.read_csv(\"./building_metadata.csv\")\n",
    "weather_train = pd.read_csv(\"./weather_train.csv\")\n",
    "train = pd.read_feather(\"./train_cleanup_001.feather\")\n",
    "\n",
    "train = train.merge(building_df, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\n",
    "train = train.merge(weather_train, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"])\n",
    "del weather_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train.loc[(train['meter']==0) & (train['site_id']==0) & (train['timestamp']<'2016-05-21 00:00:00'), 'drop'] = True\n",
    "train = train[train['drop']!=True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\n",
    "train[\"hour\"] = train[\"timestamp\"].dt.hour\n",
    "train[\"weekday\"] = train[\"timestamp\"].dt.weekday\n",
    "\n",
    "train = average_imputation(train, 'wind_speed')\n",
    "\n",
    "beaufort = [(0, 0, 0.3), (1, 0.3, 1.6), (2, 1.6, 3.4), (3, 3.4, 5.5), (4, 5.5, 8), (5, 8, 10.8), (6, 10.8, 13.9), \n",
    "          (7, 13.9, 17.2), (8, 17.2, 20.8), (9, 20.8, 24.5), (10, 24.5, 28.5), (11, 28.5, 33), (12, 33, 200)]\n",
    "\n",
    "for item in beaufort:\n",
    "    train.loc[(train['wind_speed']>=item[1]) & (train['wind_speed']<item[2]), 'beaufort_scale'] = item[0]\n",
    "\n",
    "del train[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on this great kernel https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != object:  # Exclude strings            \n",
    "            # Print current column type\n",
    "            print(\"******************************\")\n",
    "            print(\"Column: \",col)\n",
    "            print(\"dtype before: \",df[col].dtype)            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = df[col].max()\n",
    "            mn = df[col].min()\n",
    "            meancol = df[col].mean()\n",
    "            print(\"min for this col: \",mn)\n",
    "            print(\"max for this col: \",mx)\n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(df[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                df[col].fillna(meancol,inplace=True)  \n",
    "                \n",
    "                print('change for', meancol)\n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = df[col].fillna(0).astype(np.int64)\n",
    "            result = (df[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        df[col] = df[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        df[col] = df[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        df[col] = df[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)    \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            \n",
    "            # Print new column type\n",
    "            print(\"dtype after: \",df[col].dtype)\n",
    "            print(\"******************************\")\n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return df, NAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train[\"primary_use\"] = le.fit_transform(train[\"primary_use\"])\n",
    "\n",
    "categoricals = [\"site_id\", \"building_id\", \"primary_use\", \"hour\", \"weekday\",  \"meter\"]\n",
    "\n",
    "drop_cols = [\"sea_level_pressure\", \"wind_speed\", \"wind_direction\"]\n",
    "\n",
    "numericals = [\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\",\n",
    "              \"dew_temperature\", \"precip_depth_1_hr\", \"floor_count\", 'beaufort_scale']\n",
    "\n",
    "feat_cols = categoricals + numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.log1p(train[\"meter_reading\"])\n",
    "\n",
    "del train[\"meter_reading\"] \n",
    "\n",
    "train = train.drop(drop_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 3033.1971435546875  MB\n",
      "******************************\n",
      "Column:  building_id\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  1448\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  meter\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  3\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  site_id\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  15\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  square_feet\n",
      "dtype before:  int64\n",
      "min for this col:  283\n",
      "max for this col:  875000\n",
      "dtype after:  uint32\n",
      "******************************\n",
      "******************************\n",
      "Column:  year_built\n",
      "dtype before:  float64\n",
      "min for this col:  1900.0\n",
      "max for this col:  2017.0\n",
      "change for 1967.0896632561887\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  floor_count\n",
      "dtype before:  float64\n",
      "min for this col:  1.0\n",
      "max for this col:  26.0\n",
      "change for 4.1275720129163735\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  air_temperature\n",
      "dtype before:  float64\n",
      "min for this col:  -28.9\n",
      "max for this col:  47.2\n",
      "change for 15.892317471254808\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  cloud_coverage\n",
      "dtype before:  float64\n",
      "min for this col:  0.0\n",
      "max for this col:  9.0\n",
      "change for 1.8927767298442322\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  dew_temperature\n",
      "dtype before:  float64\n",
      "min for this col:  -35.0\n",
      "max for this col:  26.1\n",
      "change for 7.618798497952911\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  precip_depth_1_hr\n",
      "dtype before:  float64\n",
      "min for this col:  -1.0\n",
      "max for this col:  343.0\n",
      "change for 0.7906748946516636\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  hour\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  23\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  weekday\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  6\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  beaufort_scale\n",
      "dtype before:  float64\n",
      "min for this col:  0.0\n",
      "max for this col:  8.0\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  1743.1143083572388  MB\n",
      "This is  57.467887046551645 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "train, NAlist = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(dense_dim_1=64, dense_dim_2=32, dense_dim_3=32, dense_dim_4=16, \n",
    "dropout1=0.2, dropout2=0.1, dropout3=0.1, dropout4=0.1, lr=0.001):\n",
    "\n",
    "    #Inputs\n",
    "    site_id = Input(shape=[1], name=\"site_id\")\n",
    "    building_id = Input(shape=[1], name=\"building_id\")\n",
    "    meter = Input(shape=[1], name=\"meter\")\n",
    "    primary_use = Input(shape=[1], name=\"primary_use\")\n",
    "    square_feet = Input(shape=[1], name=\"square_feet\")\n",
    "    year_built = Input(shape=[1], name=\"year_built\")\n",
    "    air_temperature = Input(shape=[1], name=\"air_temperature\")\n",
    "    cloud_coverage = Input(shape=[1], name=\"cloud_coverage\")\n",
    "    dew_temperature = Input(shape=[1], name=\"dew_temperature\")\n",
    "    hour = Input(shape=[1], name=\"hour\")\n",
    "    precip = Input(shape=[1], name=\"precip_depth_1_hr\")\n",
    "    weekday = Input(shape=[1], name=\"weekday\")\n",
    "    beaufort_scale = Input(shape=[1], name=\"beaufort_scale\")\n",
    "   \n",
    "    #Embeddings layers\n",
    "    emb_site_id = Embedding(16, 2)(site_id)\n",
    "    emb_building_id = Embedding(1449, 6)(building_id)\n",
    "    emb_meter = Embedding(4, 2)(meter)\n",
    "    emb_primary_use = Embedding(16, 2)(primary_use)\n",
    "    emb_hour = Embedding(24, 3)(hour)\n",
    "    emb_weekday = Embedding(7, 2)(weekday)\n",
    "\n",
    "    concat_emb = concatenate([\n",
    "           Flatten() (emb_site_id)\n",
    "         , Flatten() (emb_building_id)\n",
    "         , Flatten() (emb_meter)\n",
    "         , Flatten() (emb_primary_use)\n",
    "         , Flatten() (emb_hour)\n",
    "         , Flatten() (emb_weekday)\n",
    "    ])\n",
    "    \n",
    "    categ = Dropout(dropout1)(Dense(dense_dim_1,activation='relu') (concat_emb))\n",
    "    categ = BatchNormalization()(categ)\n",
    "    categ = Dropout(dropout2)(Dense(dense_dim_2,activation='relu') (categ))\n",
    "    \n",
    "    #main layer\n",
    "    main_l = concatenate([\n",
    "          categ\n",
    "        , square_feet\n",
    "        , year_built\n",
    "        , air_temperature\n",
    "        , cloud_coverage\n",
    "        , dew_temperature\n",
    "        , precip\n",
    "        , beaufort_scale\n",
    "    ])\n",
    "    \n",
    "    main_l = Dropout(dropout3)(Dense(dense_dim_3,activation='relu') (main_l))\n",
    "    main_l = BatchNormalization()(main_l)\n",
    "    \n",
    "    main_2 = concatenate([\n",
    "          main_l\n",
    "        , Flatten() (emb_building_id)\n",
    "        , Flatten() (emb_site_id)\n",
    "\n",
    "    ])\n",
    "    \n",
    "    main_2 = Dropout(dropout4)(Dense(dense_dim_4,activation='relu') (main_2))\n",
    "    \n",
    "    #output\n",
    "    output = Dense(1) (main_2)\n",
    "\n",
    "    model = Model([ site_id,\n",
    "                    building_id, \n",
    "                    meter, \n",
    "                    primary_use, \n",
    "                    square_feet, \n",
    "                    year_built, \n",
    "                   \n",
    "                    air_temperature,\n",
    "                    cloud_coverage,\n",
    "                    dew_temperature, \n",
    "                    hour,\n",
    "                    weekday, \n",
    "                    precip,\n",
    "                    beaufort_scale], output)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr=lr),\n",
    "                  loss= mse_loss,\n",
    "                  metrics=[root_mean_squared_error])\n",
    "    return model\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_data(df, num_cols, cat_cols):\n",
    "    cols = cat_cols + num_cols\n",
    "    X = {col: np.array(df[col]) for col in cols}\n",
    "    return X\n",
    "\n",
    "def train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold, patience=3):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=1)\n",
    "    model_checkpoint = ModelCheckpoint(\"model_\" + str(fold) + \".hdf5\",\n",
    "                                       save_best_only=True, verbose=1, monitor='val_root_mean_squared_error', mode='min')\n",
    "\n",
    "    hist = keras_model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                            validation_data=(X_v, y_valid), verbose=1,\n",
    "                            callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "    keras_model = load_model(\"model_\" + str(fold) + \".hdf5\", custom_objects={'root_mean_squared_error': root_mean_squared_error})\n",
    "    \n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Train on 9802217 samples, validate on 9802854 samples\n",
      "Epoch 1/15\n",
      "9802217/9802217 [==============================] - 263s 27us/step - loss: 1.4013 - root_mean_squared_error: 1.1675 - val_loss: 1.4058 - val_root_mean_squared_error: 1.1214\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 1.12139, saving model to model_0.hdf5\n",
      "Epoch 2/15\n",
      "9802217/9802217 [==============================] - 267s 27us/step - loss: 0.9790 - root_mean_squared_error: 0.9833 - val_loss: 1.1196 - val_root_mean_squared_error: 1.0104\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 1.12139 to 1.01044, saving model to model_0.hdf5\n",
      "Epoch 3/15\n",
      "9802217/9802217 [==============================] - 263s 27us/step - loss: 0.8935 - root_mean_squared_error: 0.9391 - val_loss: 1.0861 - val_root_mean_squared_error: 0.9947\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 1.01044 to 0.99470, saving model to model_0.hdf5\n",
      "Epoch 4/15\n",
      "9802217/9802217 [==============================] - 263s 27us/step - loss: 0.8632 - root_mean_squared_error: 0.9229 - val_loss: 1.1243 - val_root_mean_squared_error: 1.0128\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 0.99470\n",
      "Epoch 5/15\n",
      "9802217/9802217 [==============================] - 256s 26us/step - loss: 0.8464 - root_mean_squared_error: 0.9138 - val_loss: 1.0851 - val_root_mean_squared_error: 0.9903\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error improved from 0.99470 to 0.99034, saving model to model_0.hdf5\n",
      "Epoch 6/15\n",
      "9802217/9802217 [==============================] - 256s 26us/step - loss: 0.8347 - root_mean_squared_error: 0.9073 - val_loss: 1.0892 - val_root_mean_squared_error: 0.9942\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.99034\n",
      "Epoch 7/15\n",
      "9802217/9802217 [==============================] - 256s 26us/step - loss: 0.8267 - root_mean_squared_error: 0.9028 - val_loss: 1.0735 - val_root_mean_squared_error: 0.9865\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error improved from 0.99034 to 0.98647, saving model to model_0.hdf5\n",
      "Epoch 8/15\n",
      "9802217/9802217 [==============================] - 256s 26us/step - loss: 0.8185 - root_mean_squared_error: 0.8983 - val_loss: 1.0796 - val_root_mean_squared_error: 0.9878\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.98647\n",
      "Epoch 9/15\n",
      "9802217/9802217 [==============================] - 256s 26us/step - loss: 0.8106 - root_mean_squared_error: 0.8939 - val_loss: 1.0675 - val_root_mean_squared_error: 0.9837\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error improved from 0.98647 to 0.98368, saving model to model_0.hdf5\n",
      "Epoch 10/15\n",
      "9802217/9802217 [==============================] - 255s 26us/step - loss: 0.8052 - root_mean_squared_error: 0.8909 - val_loss: 1.0577 - val_root_mean_squared_error: 0.9782\n",
      "\n",
      "Epoch 00010: val_root_mean_squared_error improved from 0.98368 to 0.97822, saving model to model_0.hdf5\n",
      "Epoch 11/15\n",
      "9802217/9802217 [==============================] - 256s 26us/step - loss: 0.8005 - root_mean_squared_error: 0.8883 - val_loss: 1.1073 - val_root_mean_squared_error: 0.9996\n",
      "\n",
      "Epoch 00011: val_root_mean_squared_error did not improve from 0.97822\n",
      "Epoch 12/15\n",
      "9802217/9802217 [==============================] - 256s 26us/step - loss: 0.7978 - root_mean_squared_error: 0.8867 - val_loss: 1.0434 - val_root_mean_squared_error: 0.9730\n",
      "\n",
      "Epoch 00012: val_root_mean_squared_error improved from 0.97822 to 0.97299, saving model to model_0.hdf5\n",
      "Epoch 13/15\n",
      "9802217/9802217 [==============================] - 256s 26us/step - loss: 0.7934 - root_mean_squared_error: 0.8841 - val_loss: 1.0914 - val_root_mean_squared_error: 0.9913\n",
      "\n",
      "Epoch 00013: val_root_mean_squared_error did not improve from 0.97299\n",
      "Epoch 14/15\n",
      "9802217/9802217 [==============================] - 256s 26us/step - loss: 0.7917 - root_mean_squared_error: 0.8833 - val_loss: 1.0441 - val_root_mean_squared_error: 0.9733\n",
      "\n",
      "Epoch 00014: val_root_mean_squared_error did not improve from 0.97299\n",
      "Epoch 15/15\n",
      "9802217/9802217 [==============================] - 258s 26us/step - loss: 0.7892 - root_mean_squared_error: 0.8818 - val_loss: 1.0684 - val_root_mean_squared_error: 0.9832\n",
      "\n",
      "Epoch 00015: val_root_mean_squared_error did not improve from 0.97299\n",
      "Epoch 00015: early stopping\n",
      "**************************************************\n",
      "Fold: 1\n",
      "Train on 9802854 samples, validate on 9802217 samples\n",
      "Epoch 1/15\n",
      "9802854/9802854 [==============================] - 273s 28us/step - loss: 1.3395 - root_mean_squared_error: 1.1353 - val_loss: 1.5246 - val_root_mean_squared_error: 1.1645\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 1.16448, saving model to model_1.hdf5\n",
      "Epoch 2/15\n",
      "9802854/9802854 [==============================] - 270s 28us/step - loss: 0.9602 - root_mean_squared_error: 0.9742 - val_loss: 1.1936 - val_root_mean_squared_error: 1.0451\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 1.16448 to 1.04506, saving model to model_1.hdf5\n",
      "Epoch 3/15\n",
      "9802854/9802854 [==============================] - 270s 28us/step - loss: 0.9144 - root_mean_squared_error: 0.9507 - val_loss: 1.1361 - val_root_mean_squared_error: 1.0199\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 1.04506 to 1.01987, saving model to model_1.hdf5\n",
      "Epoch 4/15\n",
      "9802854/9802854 [==============================] - 270s 28us/step - loss: 0.8787 - root_mean_squared_error: 0.9319 - val_loss: 1.1053 - val_root_mean_squared_error: 1.0037\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error improved from 1.01987 to 1.00367, saving model to model_1.hdf5\n",
      "Epoch 5/15\n",
      "9802854/9802854 [==============================] - 270s 28us/step - loss: 0.8579 - root_mean_squared_error: 0.9208 - val_loss: 1.0943 - val_root_mean_squared_error: 0.9987\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error improved from 1.00367 to 0.99869, saving model to model_1.hdf5\n",
      "Epoch 6/15\n",
      "9802854/9802854 [==============================] - 276s 28us/step - loss: 0.8440 - root_mean_squared_error: 0.9131 - val_loss: 1.0672 - val_root_mean_squared_error: 0.9859\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error improved from 0.99869 to 0.98586, saving model to model_1.hdf5\n",
      "Epoch 7/15\n",
      "9802854/9802854 [==============================] - 278s 28us/step - loss: 0.8327 - root_mean_squared_error: 0.9070 - val_loss: 1.0848 - val_root_mean_squared_error: 0.9953\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.98586\n",
      "Epoch 8/15\n",
      "9802854/9802854 [==============================] - 275s 28us/step - loss: 0.8221 - root_mean_squared_error: 0.9011 - val_loss: 1.0686 - val_root_mean_squared_error: 0.9858\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error improved from 0.98586 to 0.98577, saving model to model_1.hdf5\n",
      "Epoch 9/15\n",
      "9802854/9802854 [==============================] - 267s 27us/step - loss: 0.8119 - root_mean_squared_error: 0.8954 - val_loss: 1.0614 - val_root_mean_squared_error: 0.9822\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error improved from 0.98577 to 0.98223, saving model to model_1.hdf5\n",
      "Epoch 10/15\n",
      "9802854/9802854 [==============================] - 268s 27us/step - loss: 0.8050 - root_mean_squared_error: 0.8915 - val_loss: 1.0570 - val_root_mean_squared_error: 0.9822\n",
      "\n",
      "Epoch 00010: val_root_mean_squared_error improved from 0.98223 to 0.98221, saving model to model_1.hdf5\n",
      "Epoch 11/15\n",
      "9802854/9802854 [==============================] - 257s 26us/step - loss: 0.8000 - root_mean_squared_error: 0.8888 - val_loss: 1.0528 - val_root_mean_squared_error: 0.9783\n",
      "\n",
      "Epoch 00011: val_root_mean_squared_error improved from 0.98221 to 0.97835, saving model to model_1.hdf5\n",
      "Epoch 12/15\n",
      "9802854/9802854 [==============================] - 402s 41us/step - loss: 0.7936 - root_mean_squared_error: 0.8851 - val_loss: 1.0350 - val_root_mean_squared_error: 0.9707\n",
      "\n",
      "Epoch 00012: val_root_mean_squared_error improved from 0.97835 to 0.97069, saving model to model_1.hdf5\n",
      "Epoch 13/15\n",
      "9802854/9802854 [==============================] - 547s 56us/step - loss: 0.7910 - root_mean_squared_error: 0.8837 - val_loss: 1.0308 - val_root_mean_squared_error: 0.9679\n",
      "\n",
      "Epoch 00013: val_root_mean_squared_error improved from 0.97069 to 0.96794, saving model to model_1.hdf5\n",
      "Epoch 14/15\n",
      "9802854/9802854 [==============================] - 547s 56us/step - loss: 0.7866 - root_mean_squared_error: 0.8813 - val_loss: 1.1294 - val_root_mean_squared_error: 1.0107\n",
      "\n",
      "Epoch 00014: val_root_mean_squared_error did not improve from 0.96794\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9802854/9802854 [==============================] - 531s 54us/step - loss: 0.7833 - root_mean_squared_error: 0.8793 - val_loss: 1.0848 - val_root_mean_squared_error: 0.9926\n",
      "\n",
      "Epoch 00015: val_root_mean_squared_error did not improve from 0.96794\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "oof = np.zeros(len(train))\n",
    "batch_size = 256\n",
    "epochs = 15\n",
    "models = []\n",
    "\n",
    "folds = 2\n",
    "seed = 666\n",
    "\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=False, random_state=seed)\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(kf.split(train, train['building_id'])):\n",
    "    print('Fold:', fold_n)\n",
    "    X_train, X_valid = train.iloc[train_index], train.iloc[valid_index]\n",
    "    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n",
    "    X_t = get_keras_data(X_train, numericals, categoricals)\n",
    "    X_v = get_keras_data(X_valid, numericals, categoricals)\n",
    "    \n",
    "    keras_model = model(dense_dim_1=64, dense_dim_2=32, dense_dim_3=32, dense_dim_4=16, \n",
    "                        dropout1=0.2, dropout2=0.1, dropout3=0.1, dropout4=0.1, lr=0.001)\n",
    "    mod = train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold_n, patience=3)\n",
    "    models.append(mod)\n",
    "    print('*'* 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Train on 9802217 samples, validate on 9802854 samples\n",
      "Epoch 1/15\n",
      "9802217/9802217 [==============================] - 169s 17us/step - loss: 1.4258 - root_mean_squared_error: 1.1737 - val_loss: 1.3367 - val_root_mean_squared_error: 1.0953\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 1.09530, saving model to model_0.hdf5\n",
      "Epoch 2/15\n",
      "9802217/9802217 [==============================] - 169s 17us/step - loss: 0.9632 - root_mean_squared_error: 0.9753 - val_loss: 1.1054 - val_root_mean_squared_error: 1.0052\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 1.09530 to 1.00519, saving model to model_0.hdf5\n",
      "Epoch 3/15\n",
      "9802217/9802217 [==============================] - 167s 17us/step - loss: 0.9015 - root_mean_squared_error: 0.9434 - val_loss: 1.1049 - val_root_mean_squared_error: 1.0038\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 1.00519 to 1.00382, saving model to model_0.hdf5\n",
      "Epoch 4/15\n",
      "9802217/9802217 [==============================] - 169s 17us/step - loss: 0.8677 - root_mean_squared_error: 0.9254 - val_loss: 1.1265 - val_root_mean_squared_error: 1.0150\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 1.00382\n",
      "Epoch 5/15\n",
      "9802217/9802217 [==============================] - 167s 17us/step - loss: 0.8484 - root_mean_squared_error: 0.9149 - val_loss: 1.0860 - val_root_mean_squared_error: 0.9938\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error improved from 1.00382 to 0.99381, saving model to model_0.hdf5\n",
      "Epoch 6/15\n",
      "9802217/9802217 [==============================] - 166s 17us/step - loss: 0.8356 - root_mean_squared_error: 0.9079 - val_loss: 1.0825 - val_root_mean_squared_error: 0.9917\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error improved from 0.99381 to 0.99170, saving model to model_0.hdf5\n",
      "Epoch 7/15\n",
      "9802217/9802217 [==============================] - 166s 17us/step - loss: 0.8224 - root_mean_squared_error: 0.9006 - val_loss: 1.0724 - val_root_mean_squared_error: 0.9876\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error improved from 0.99170 to 0.98757, saving model to model_0.hdf5\n",
      "Epoch 8/15\n",
      "9802217/9802217 [==============================] - 167s 17us/step - loss: 0.8144 - root_mean_squared_error: 0.8962 - val_loss: 1.1164 - val_root_mean_squared_error: 1.0086\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.98757\n",
      "Epoch 9/15\n",
      "9802217/9802217 [==============================] - 166s 17us/step - loss: 0.8082 - root_mean_squared_error: 0.8927 - val_loss: 1.0798 - val_root_mean_squared_error: 0.9899\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error did not improve from 0.98757\n",
      "Epoch 10/15\n",
      "9802217/9802217 [==============================] - 168s 17us/step - loss: 0.8041 - root_mean_squared_error: 0.8903 - val_loss: 1.0919 - val_root_mean_squared_error: 0.9935\n",
      "\n",
      "Epoch 00010: val_root_mean_squared_error did not improve from 0.98757\n",
      "Epoch 00010: early stopping\n",
      "**************************************************\n",
      "Fold: 1\n",
      "Train on 9802854 samples, validate on 9802217 samples\n",
      "Epoch 1/15\n",
      "9802854/9802854 [==============================] - 176s 18us/step - loss: 1.3065 - root_mean_squared_error: 1.1248 - val_loss: 1.3992 - val_root_mean_squared_error: 1.1282\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 1.12818, saving model to model_1.hdf5\n",
      "Epoch 2/15\n",
      "9802854/9802854 [==============================] - 172s 18us/step - loss: 0.9123 - root_mean_squared_error: 0.9496 - val_loss: 1.2571 - val_root_mean_squared_error: 1.0628\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 1.12818 to 1.06280, saving model to model_1.hdf5\n",
      "Epoch 3/15\n",
      "9802854/9802854 [==============================] - 171s 17us/step - loss: 0.8313 - root_mean_squared_error: 0.9061 - val_loss: 1.1368 - val_root_mean_squared_error: 1.0165\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 1.06280 to 1.01655, saving model to model_1.hdf5\n",
      "Epoch 4/15\n",
      "9802854/9802854 [==============================] - 172s 18us/step - loss: 0.8000 - root_mean_squared_error: 0.8886 - val_loss: 1.1729 - val_root_mean_squared_error: 1.0325\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error did not improve from 1.01655\n",
      "Epoch 5/15\n",
      "9802854/9802854 [==============================] - 175s 18us/step - loss: 0.7845 - root_mean_squared_error: 0.8797 - val_loss: 1.0406 - val_root_mean_squared_error: 0.9724\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error improved from 1.01655 to 0.97241, saving model to model_1.hdf5\n",
      "Epoch 6/15\n",
      "9802854/9802854 [==============================] - 172s 18us/step - loss: 0.7715 - root_mean_squared_error: 0.8723 - val_loss: 1.0364 - val_root_mean_squared_error: 0.9712\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error improved from 0.97241 to 0.97124, saving model to model_1.hdf5\n",
      "Epoch 7/15\n",
      "9802854/9802854 [==============================] - 173s 18us/step - loss: 0.7633 - root_mean_squared_error: 0.8677 - val_loss: 1.0693 - val_root_mean_squared_error: 0.9858\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.97124\n",
      "Epoch 8/15\n",
      "9802854/9802854 [==============================] - 173s 18us/step - loss: 0.7574 - root_mean_squared_error: 0.8643 - val_loss: 1.0387 - val_root_mean_squared_error: 0.9720\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.97124\n",
      "Epoch 9/15\n",
      "9802854/9802854 [==============================] - 171s 17us/step - loss: 0.7513 - root_mean_squared_error: 0.8607 - val_loss: 1.0405 - val_root_mean_squared_error: 0.9709\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error improved from 0.97124 to 0.97094, saving model to model_1.hdf5\n",
      "Epoch 00009: early stopping\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "oof = np.zeros(len(train))\n",
    "batch_size = 256\n",
    "epochs = 15\n",
    "models = []\n",
    "\n",
    "folds = 2\n",
    "seed = 666\n",
    "\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=False, random_state=seed)\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(kf.split(train, train['building_id'])):\n",
    "    print('Fold:', fold_n)\n",
    "    X_train, X_valid = train.iloc[train_index], train.iloc[valid_index]\n",
    "    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n",
    "    X_t = get_keras_data(X_train, numericals, categoricals)\n",
    "    X_v = get_keras_data(X_valid, numericals, categoricals)\n",
    "    \n",
    "    keras_model = model(dense_dim_1=64, dense_dim_2=32, dense_dim_3=32, dense_dim_4=16, \n",
    "                        dropout1=0.2, dropout2=0.1, dropout3=0.1, dropout4=0.1, lr=0.001)\n",
    "    mod = train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold_n, patience=3)\n",
    "    models.append(mod)\n",
    "    print('*'* 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['square_feet',\n",
       " 'year_built',\n",
       " 'air_temperature',\n",
       " 'cloud_coverage',\n",
       " 'dew_temperature',\n",
       " 'precip_depth_1_hr',\n",
       " 'floor_count',\n",
       " 'beaufort_scale']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del train, target, X_train, X_valid, y_train, y_valid, X_t, X_v, kf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./test.csv\")\n",
    "test = test.merge(building_df, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\n",
    "del building_df\n",
    "gc.collect()\n",
    "test[\"primary_use\"] = le.transform(test[\"primary_use\"])\n",
    "\n",
    "weather_test = pd.read_csv(\"./weather_test.csv\")\n",
    "\n",
    "test = test.merge(weather_test, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"], how = \"left\")\n",
    "del weather_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 6051.91162109375  MB\n",
      "******************************\n",
      "Column:  site_id\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  15\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  building_id\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  1448\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  primary_use\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  15\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  hour\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  23\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  weekday\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  6\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  meter\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  3\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  square_feet\n",
      "dtype before:  int64\n",
      "min for this col:  283\n",
      "max for this col:  875000\n",
      "dtype after:  uint32\n",
      "******************************\n",
      "******************************\n",
      "Column:  year_built\n",
      "dtype before:  float64\n",
      "min for this col:  1900.0\n",
      "max for this col:  2017.0\n",
      "change for 1968.170081967213\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  air_temperature\n",
      "dtype before:  float64\n",
      "min for this col:  -28.1\n",
      "max for this col:  48.3\n",
      "change for 15.505707129926357\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  cloud_coverage\n",
      "dtype before:  float64\n",
      "min for this col:  0.0\n",
      "max for this col:  9.0\n",
      "change for 1.9733458449444876\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  dew_temperature\n",
      "dtype before:  float64\n",
      "min for this col:  -31.6\n",
      "max for this col:  26.7\n",
      "change for 7.585971305989484\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  precip_depth_1_hr\n",
      "dtype before:  float64\n",
      "min for this col:  -1.0\n",
      "max for this col:  597.0\n",
      "change for 0.9182989150029545\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  floor_count\n",
      "dtype before:  float64\n",
      "min for this col:  1.0\n",
      "max for this col:  26.0\n",
      "change for 4.120772946859903\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  beaufort_scale\n",
      "dtype before:  float64\n",
      "min for this col:  0.0\n",
      "max for this col:  9.0\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  3029.700927734375  MB\n",
      "This is  50.061883210165306 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "test[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\n",
    "test[\"hour\"] = test[\"timestamp\"].dt.hour\n",
    "test[\"weekday\"] = test[\"timestamp\"].dt.weekday\n",
    "\n",
    "test = average_imputation(test, 'wind_speed')\n",
    "\n",
    "for item in beaufort:\n",
    "    test.loc[(test['wind_speed']>=item[1]) & (test['wind_speed']<item[2]), 'beaufort_scale'] = item[0]\n",
    "\n",
    "    \n",
    "test = test[feat_cols]\n",
    "test, NAlist = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 834/834 [09:58<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "i=0\n",
    "res = np.zeros((test.shape[0]),dtype=np.float32)\n",
    "step_size = 50000\n",
    "for j in tqdm(range(int(np.ceil(test.shape[0]/step_size)))):\n",
    "    for_prediction = get_keras_data(test.iloc[i:i+step_size], numericals, categoricals)\n",
    "    res[i:min(i+step_size,test.shape[0])] = \\\n",
    "       np.expm1(sum([model.predict(for_prediction, batch_size=1024)[:,0] for model in models])/folds)\n",
    "    i+=step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>197.040619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>96.668510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9.802197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>313.558533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1295.750488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697595</td>\n",
       "      <td>41697595</td>\n",
       "      <td>7.001263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697596</td>\n",
       "      <td>41697596</td>\n",
       "      <td>4.312058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697597</td>\n",
       "      <td>41697597</td>\n",
       "      <td>4.615134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697598</td>\n",
       "      <td>41697598</td>\n",
       "      <td>172.538712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697599</td>\n",
       "      <td>41697599</td>\n",
       "      <td>3.284245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0     197.040619\n",
       "1                1      96.668510\n",
       "2                2       9.802197\n",
       "3                3     313.558533\n",
       "4                4    1295.750488\n",
       "...            ...            ...\n",
       "41697595  41697595       7.001263\n",
       "41697596  41697596       4.312058\n",
       "41697597  41697597       4.615134\n",
       "41697598  41697598     172.538712\n",
       "41697599  41697599       3.284245\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['meter_reading'] = res\n",
    "submission.loc[submission['meter_reading']<0, 'meter_reading'] = 0\n",
    "submission.to_csv('submission_nn001.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ubuntu/.kaggle/kaggle.json'\n",
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
      "100%|| 732M/732M [00:27<00:00, 27.8MB/s]\n",
      "Successfully submitted to ASHRAE - Great Energy Predictor III"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c ashrae-energy-prediction -f submission_nn001.csv -m \"Dense NN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
